# -*- coding: utf-8 -*-
"""Text Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C4xJFPb1pJ8TF2REEdl3PkxIDxOtimeq

# Análisis de Texto para Prima

- El objetivo de este notebook es descubrir insights sobre la conversación entorno a la marca de **Prima** por medio de análisis de texto.

# 1.Librerias
"""

!pip install ftfy -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from wordcloud import WordCloud

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 40)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.simplefilter(action='ignore', category=FutureWarning)

"""# 2.Carga del dataset"""

path = '/content/drive/MyDrive/(Analytics) Projects/11. Text Analysis for Prima/data/bbdd_consolidado_prima - quantico_sentimiento_prima.csv'

raw_data = pd.read_csv(path)

raw_data.shape

"""# 3.Filtrado del dataset"""

import ftfy

def clean_and_filter_dataset(dataframe):
    df = dataframe.copy()
    df = df.dropna(subset=['mensaje'])
    df = df.dropna(subset=['fecha'])

    df['id'] = df['id'].astype('string')

    df['hora'] = df['hora'].apply(lambda x: str(x).split(':')[0])
    df['mensaje'] = df['mensaje'].apply(lambda x: ftfy.fix_text(str(x)))
    df['mensaje'] = df['mensaje'].apply(lambda x: x.lower())
    df['anio'] = df['fecha'].apply(lambda x: str(x)[:4])
    df['mes'] = df['fecha'].apply(lambda x: str(x)[4:6])
    print('='*20, 'TRANSFORMACIONES FINALIZADAS', '='*20)
    print()

    df = df[df['anio']!='http'] # se vuela un único registro mal almacenado
    print('='*20, 'LIMPIEZA TERMINADA', '='*20)
    print()

    df['anio'] = df['anio'].astype('int')
    df['mes'] = df['mes'].astype('int')
    print('='*20, 'CONVERSIONES FINALIZADAS', '='*20)
    print()

    mask_1 = df['anio'] == 2023
    mask_2 = df['mes'] >= 6
    df = df.loc[mask_1 & mask_2]
    print('='*20, 'PROCESO TERMINADO', '='*20)
    print()

    return df

data = clean_and_filter_dataset(raw_data)

data.shape

"""# 4.Análisis del texto

## 4.1.Contexto General
"""

!cp /content/drive/MyDrive/(Analytics) Projects/00. modules/text_analysis/text_analytics_tools.py

"""
Nombre del Módulo: text_analytics_tools
Versión: 1.0.0
Descripción: Módulo de Python con funciones personalizadas para análisis de textos

Desarrollado por:
  Cesar Chalco Elias
  Correo Electrónico: cchalco@tbwaperu.com.com

Copyright (c) 2024 Cesar Chalco Elias. Todos los derechos reservados.

Fecha de Creación: Enero 2024
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def grafico_barras_apiladas_subplot(ax, df, apertura, title):
    """
    Genera un gráfico de barras apiladas en un subplot.

    Parámetros:
    - ax (matplotlib.axes._subplots.AxesSubplot): Subplot donde se dibujará el gráfico.
    - df (pandas.DataFrame): DataFrame que contiene los datos para el gráfico.
    - apertura (str): Nombre de la columna en df que se utilizará para apilar las barras.
    - title (str): Título del gráfico.

    Nota: La función asume que df tiene una columna 'mes' que se utilizará como eje x.

    """
    # Paso 1: Crear un DataFrame pivote para los datos
    pivot_df = df.pivot_table(index='mes', columns=str(apertura), aggfunc='size', fill_value=0)

    # Paso 2: Calcular el porcentaje de cada categoría para cada mes
    total_por_mes = pivot_df.sum(axis=1)
    pivot_df = pivot_df.div(total_por_mes, axis=0) * 100

    # Paso 3: Generar el gráfico de barras apiladas
    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax, width=0.8)

    # Configuración del gráfico
    ax.set_xlabel('Mes')
    ax.set_ylabel('Porcentaje')
    ax.set_title(f'{title}')
    ax.legend(title='Valoración', loc='center left', bbox_to_anchor=(1, 0.5))

    # Añadir etiquetas de porcentaje en cada barra
    for bar in bars.patches:
        height = bar.get_height()
        label_y = bar.get_y() + height / 2
        ax.annotate(f'{height:.0f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, label_y),
                    ha='center', va='center',
                    xytext=(0, 3), textcoords='offset points',
                    fontsize=10)

fig, ax = plt.subplots(figsize=(10, 4))
grafico_barras_apiladas_subplot(ax, data,'valoracion', 'Evolución del sentimiento en Q3 y Q4 (2023)')
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(10, 4))
grafico_barras_apiladas_subplot(ax, data,'medio', 'Evolutivo de proporción por red social Q3 y Q4 (2023)')
plt.tight_layout()
plt.show()

"""## 4.2.Análisis"""

!pip install nltk -q

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer

stoplist = stopwords.words('spanish')  + ['si', 'nan']

def plot_n_grams(dataframe,
                 n_gram,
                 title,
                 n_words=10,
                 text_column='mensaje'):
    c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(n_gram, n_gram))
    ngrams = c_vec.fit_transform(dataframe[text_column])
    count_values = ngrams.toarray().sum(axis=0)
    vocab = c_vec.vocabulary_

    df_ngram = pd.DataFrame(sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True)).rename(
        columns={0: 'frequency', 1: 'n_grams'})

    df_ngram = df_ngram.sort_values(by='frequency', ascending=False)
    pd.set_option('display.max_colwidth', 50)

    li_words = df_ngram.head(n_words).n_grams.values

    plt.figure(figsize=(5, 5))
    plot = sns.barplot(x='frequency', y='n_grams', data=df_ngram.head(n_words), palette='RdYlBu')
    plot.set_xlabel('')
    plot.set_ylabel('')
    plt.title(title)
    plt.show()

def get_samples(dataframe, text_column, word):
    """
    Filter dataframe records based on a specific word in a text column.

    Parameters:
    - dataframe (pd.DataFrame): The input DataFrame.
    - text_column (str): The name of the text column in the DataFrame.
    - word (str): The target word to filter records.

    Returns:
    - pd.DataFrame: A DataFrame containing only the records that match the criteria.
    """
    filtered_df = dataframe[dataframe[text_column].str.contains(fr'\b{word}\b', case=False, regex=True)]
    return filtered_df

"""### 4.2.1. Monopalabra general + negativo + neutro"""

plot_n_grams(
    dataframe=data,
    n_gram=1,
    title='Palabras más frecuentes en PRIMA (Global)',
    n_words=10
)

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Negativa'],
    n_gram=1,
    title='Palabras más frecuentes en PRIMA (NEGATIVOS)',
    n_words=10
)

"""- De forma general hay escontento por problemas con "devolución de dinero":
- Resaltan palabras como:
    - Devuelvan
    - Ladrones
    - Soles
"""

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Neutra'],
    n_gram=1,
    title='Palabras más frecuentes en PRIMA (NEUTROS)',
    n_words=10
)

"""- De forma general en los comentarios neutros resaltan palabras informativas como:
    - Saber
    - Quiero
    - Puedo
    - Favor (probablemente de "por favor")

### 4.2.1. Bi_gram negativo + neutro
"""

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Negativa'],
    n_gram=2,
    title='Palabras más frecuentes en PRIMA (NEGATIVOS)',
    n_words=10
)

"""Notas:
- Predominancia de conversación en torno a "devolver plata".
- De mediano impacto la mención de los 17 años de Prima como AFP líder.
"""

# pd.set_option('display.max_colwidth', 150)

len(data.loc[data.valoracion=='Negativa'])

get_samples(
    dataframe=data.loc[data.valoracion=='Negativa'],
    text_column='mensaje',
    word='plazo fijo'
).mensaje.values
# ).shape
#.sample(2)

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Neutra'],
    n_gram=2,
    title='Palabras más frecuentes en PRIMA (NEUTROS)',
    n_words=10
)

"""- En el caso del sentimiento neutro:
    - Predominancia de la conversación en torno a un contenido de seguridad relacionado a Stephen Jika y a la Sra Lovina.

### 4.2.2. N_gram + Campañas principales
"""

filtered_data = data[data['categoria1'] != 'Sin_Categoria']

proporcion_categorias = filtered_data['categoria1'].value_counts(normalize=True)

cantidad_registros_por_categoria = filtered_data['categoria1'].value_counts()

print("Proporciones de categorías y cantidad de registros por categoría:\n")
print(proporcion_categorias)
print('\n','='*80)
print("\nCantidad de registros:\n")
print(cantidad_registros_por_categoria)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Campañas') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(CAMPAÑAS + NEGATIVA)',
    n_words=10
)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Campañas') & (data.valoracion=='Negativa')],
    n_gram=1,
    title='(CAMPAÑAS + NEGATIVA)',
    n_words=10
)

# get_samples(
#     dataframe=data.loc[(data.categoria1=='Campañas') & (data.valoracion=='Negativa')],
#     text_column='mensaje',
#     word='17'
# )['mensaje'].values

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Aportes y fondos') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(APORTE Y FONDOS + NEGATIVA)',
    n_words=10
)

get_samples(
    dataframe=data.loc[(data.categoria1=='Aportes y fondos') & (data.valoracion=='Negativa')],
    text_column='mensaje',
    word='fondo viendo'
)['mensaje'].values

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Retiros coyuntura') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(RETIROS + NEGATIVA)',
    n_words=10
)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Retiros coyuntura') & (data.valoracion=='Negativa')],
    n_gram=1,
    title='(RETIROS + NEGATIVA)',
    n_words=10
)

# get_samples(
#     dataframe=data.loc[(data.categoria1=='Retiros coyuntura') & (data.valoracion=='Negativa')],
#     text_column='mensaje',
#     word='plata'
# )['mensaje'].values

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Marca') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(MARCA + NEGATIVA)',
    n_words=15
)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Marca') & (data.valoracion=='Negativa')],
    n_gram=1,
    title='(MARCA + NEGATIVA)',
    n_words=15
)

"""- En marca se tiene el 56% de los comentarios totales categorizados, pero se resume en hate de múltiples tipos no tan específico en el mensaje como otras categorias."""

# get_samples(
#     dataframe=data.loc[(data.categoria1=='Marca') & (data.valoracion=='Negativa')],
#     text_column='mensaje',
#     word='todas'
# )['mensaje'].values

"""# *5.Resumen del EDA*  
---  
- Para el **sentimiento negativo** hay una gran predominancia de palabras referentes a rentabilidad y a los 17 años de Prima liderando (ejemplo: "17 años robando mi plata... sinvergüenzas" o "17 años enriqueciendo con la plata de los aportes"). Se sugiere tomar acción para cambiar la percepción de enriquecimiento a costa de los clientes que tiene la marca.
- Para el sentimiento neutro la combinación de palabras más frecuentes corresponde a solicitudes de información. Así como buena recepción de contenido de seguridad ("gracias") con la participación de Stephen Jika y Lovina Stephen.
- De las categorias tagueadas:
	- El 17% corresponde a **Campañas** en su mayoría de traspasos, donde el sentimiento negativo se ve muy concentrado en torno a los 17 años de prima liderando y cuenta con alta presencia de comentarios de poop (💩) además de resumirse en "17 años robando".
	- El 11% corresponde a **Aportes y fondos**, donde el sentimiento negativo concentra combinaciones de palabras referentes a que "la rentabilidad en realidad es negativa".
	- El 7% corresponde a **Retiros**, donde la negatividad está centrada en la devolución de dinero y que "la afp no libera las 4 UIT's que tocan por ley".
-Entre las palabras frecuentes, se destacó la combinación **plazo fijo**, donde los usuarios mencionaron su preferencia por retirar dinero de la AFP y depositarlo en un plazo fijo. Esto sugiere la oportunidad de trabajar con productos similares o reforzar la confianza que genera un Fondo de Pensiones en comparación con un producto como el plazo fijo.

"""

!pip install nltk
import nltk
nltk.download('stopwords')

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from nltk.corpus import stopwords

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto' y 'valoracion'
data_negativa = data[data['valoracion'] == 'Negativa']

# Extraer los textos
textos_negativos = data_negativa['mensaje']

# Obtener las stopwords en español de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer trigramas
vectorizer = CountVectorizer(ngram_range=(3, 3), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_negativos)
trigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_trigrams = trigrams[top_indices]

# Crear una representación de texto de trigramas
trigrams_text = ' '.join(top_trigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(trigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

data_negativa = data[data['valoracion'] == 'Negativa']

# Extraer los textos
textos_negativos = data_negativa['mensaje']

# Obtener las stopwords en español de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer bigramas
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_negativos)
bigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_bigrams = bigrams[top_indices]

# Crear una representación de texto de trigramas
bigrams_text = ' '.join(top_bigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(bigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto', 'valoracion' y 'categoria1'
data_filtrada = data[(data['valoracion'] == 'Negativa') & (data['categoria1'] == 'Retiros coyuntura')]

# Extraer los textos
textos_filtrados = data_filtrada['mensaje']

# Obtener las stopwords en español de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer trigramas
vectorizer = CountVectorizer(ngram_range=(3, 3), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_filtrados)
trigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_trigrams = trigrams[top_indices]

# Crear una representación de texto de trigramas
trigrams_text = ' '.join(top_trigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(trigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto', 'valoracion' y 'categoria1'
data_filtrada = data[(data['valoracion'] == 'Negativa') & (data['categoria1'] == 'Retiros coyuntura')]

# Extraer los textos
textos_filtrados = data_filtrada['mensaje']

# Obtener las stopwords en español de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer trigramas
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_filtrados)
bigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_bigrams = bigrams[top_indices]

# Crear una representación de texto de trigramas
bigrams_text = ' '.join(top_bigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(bigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from nltk.corpus import stopwords
import pandas as pd

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto' y 'valoracion'
data_negativa = data[data['valoracion'] == 'Negativa']

# Extraer los textos
textos_negativos = data_negativa['mensaje']

# Obtener las stopwords en español de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer bigramas
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_negativos)
bigrams = vectorizer.get_feature_names_out()
frequencies = X.toarray().sum(axis=0)

# Crear un diccionario de bigrama-frecuencia
bigram_frequency = dict(zip(bigrams, frequencies))

# Definir una función para asignar colores
def color_func(word, *args, **kwargs):
    freq = bigram_frequency.get(word, 0)
    if freq > np.percentile(frequencies, 30):
        return 'red'  # Mayor frecuencia
    elif freq > np.percentile(frequencies, 20):
        return 'orange'  # Frecuencia media
    else:
        return 'yellow'  # Menor frecuencia

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                      background_color ='white',
                      stopwords = set(stopwords_es),
                      min_font_size = 10,
                      color_func=color_func).generate_from_frequencies(bigram_frequency)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()