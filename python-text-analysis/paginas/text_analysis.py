# -*- coding: utf-8 -*-
"""Text Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C4xJFPb1pJ8TF2REEdl3PkxIDxOtimeq

# An치lisis de Texto para Prima

- El objetivo de este notebook es descubrir insights sobre la conversaci칩n entorno a la marca de **Prima** por medio de an치lisis de texto.

# 1.Librerias
"""

!pip install ftfy -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from wordcloud import WordCloud

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 40)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.simplefilter(action='ignore', category=FutureWarning)

"""# 2.Carga del dataset"""

path = '/content/drive/MyDrive/(Analytics) Projects/11. Text Analysis for Prima/data/bbdd_consolidado_prima - quantico_sentimiento_prima.csv'

raw_data = pd.read_csv(path)

raw_data.shape

"""# 3.Filtrado del dataset"""

import ftfy

def clean_and_filter_dataset(dataframe):
    df = dataframe.copy()
    df = df.dropna(subset=['mensaje'])
    df = df.dropna(subset=['fecha'])

    df['id'] = df['id'].astype('string')

    df['hora'] = df['hora'].apply(lambda x: str(x).split(':')[0])
    df['mensaje'] = df['mensaje'].apply(lambda x: ftfy.fix_text(str(x)))
    df['mensaje'] = df['mensaje'].apply(lambda x: x.lower())
    df['anio'] = df['fecha'].apply(lambda x: str(x)[:4])
    df['mes'] = df['fecha'].apply(lambda x: str(x)[4:6])
    print('='*20, 'TRANSFORMACIONES FINALIZADAS', '='*20)
    print()

    df = df[df['anio']!='http'] # se vuela un 칰nico registro mal almacenado
    print('='*20, 'LIMPIEZA TERMINADA', '='*20)
    print()

    df['anio'] = df['anio'].astype('int')
    df['mes'] = df['mes'].astype('int')
    print('='*20, 'CONVERSIONES FINALIZADAS', '='*20)
    print()

    mask_1 = df['anio'] == 2023
    mask_2 = df['mes'] >= 6
    df = df.loc[mask_1 & mask_2]
    print('='*20, 'PROCESO TERMINADO', '='*20)
    print()

    return df

data = clean_and_filter_dataset(raw_data)

data.shape

"""# 4.An치lisis del texto

## 4.1.Contexto General
"""

!cp /content/drive/MyDrive/(Analytics) Projects/00. modules/text_analysis/text_analytics_tools.py

"""
Nombre del M칩dulo: text_analytics_tools
Versi칩n: 1.0.0
Descripci칩n: M칩dulo de Python con funciones personalizadas para an치lisis de textos

Desarrollado por:
  Cesar Chalco Elias
  Correo Electr칩nico: cchalco@tbwaperu.com.com

Copyright (c) 2024 Cesar Chalco Elias. Todos los derechos reservados.

Fecha de Creaci칩n: Enero 2024
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def grafico_barras_apiladas_subplot(ax, df, apertura, title):
    """
    Genera un gr치fico de barras apiladas en un subplot.

    Par치metros:
    - ax (matplotlib.axes._subplots.AxesSubplot): Subplot donde se dibujar치 el gr치fico.
    - df (pandas.DataFrame): DataFrame que contiene los datos para el gr치fico.
    - apertura (str): Nombre de la columna en df que se utilizar치 para apilar las barras.
    - title (str): T칤tulo del gr치fico.

    Nota: La funci칩n asume que df tiene una columna 'mes' que se utilizar치 como eje x.

    """
    # Paso 1: Crear un DataFrame pivote para los datos
    pivot_df = df.pivot_table(index='mes', columns=str(apertura), aggfunc='size', fill_value=0)

    # Paso 2: Calcular el porcentaje de cada categor칤a para cada mes
    total_por_mes = pivot_df.sum(axis=1)
    pivot_df = pivot_df.div(total_por_mes, axis=0) * 100

    # Paso 3: Generar el gr치fico de barras apiladas
    bars = pivot_df.plot(kind='bar', stacked=True, ax=ax, width=0.8)

    # Configuraci칩n del gr치fico
    ax.set_xlabel('Mes')
    ax.set_ylabel('Porcentaje')
    ax.set_title(f'{title}')
    ax.legend(title='Valoraci칩n', loc='center left', bbox_to_anchor=(1, 0.5))

    # A침adir etiquetas de porcentaje en cada barra
    for bar in bars.patches:
        height = bar.get_height()
        label_y = bar.get_y() + height / 2
        ax.annotate(f'{height:.0f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, label_y),
                    ha='center', va='center',
                    xytext=(0, 3), textcoords='offset points',
                    fontsize=10)

fig, ax = plt.subplots(figsize=(10, 4))
grafico_barras_apiladas_subplot(ax, data,'valoracion', 'Evoluci칩n del sentimiento en Q3 y Q4 (2023)')
plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(10, 4))
grafico_barras_apiladas_subplot(ax, data,'medio', 'Evolutivo de proporci칩n por red social Q3 y Q4 (2023)')
plt.tight_layout()
plt.show()

"""## 4.2.An치lisis"""

!pip install nltk -q

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer

stoplist = stopwords.words('spanish')  + ['si', 'nan']

def plot_n_grams(dataframe,
                 n_gram,
                 title,
                 n_words=10,
                 text_column='mensaje'):
    c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(n_gram, n_gram))
    ngrams = c_vec.fit_transform(dataframe[text_column])
    count_values = ngrams.toarray().sum(axis=0)
    vocab = c_vec.vocabulary_

    df_ngram = pd.DataFrame(sorted([(count_values[i], k) for k, i in vocab.items()], reverse=True)).rename(
        columns={0: 'frequency', 1: 'n_grams'})

    df_ngram = df_ngram.sort_values(by='frequency', ascending=False)
    pd.set_option('display.max_colwidth', 50)

    li_words = df_ngram.head(n_words).n_grams.values

    plt.figure(figsize=(5, 5))
    plot = sns.barplot(x='frequency', y='n_grams', data=df_ngram.head(n_words), palette='RdYlBu')
    plot.set_xlabel('')
    plot.set_ylabel('')
    plt.title(title)
    plt.show()

def get_samples(dataframe, text_column, word):
    """
    Filter dataframe records based on a specific word in a text column.

    Parameters:
    - dataframe (pd.DataFrame): The input DataFrame.
    - text_column (str): The name of the text column in the DataFrame.
    - word (str): The target word to filter records.

    Returns:
    - pd.DataFrame: A DataFrame containing only the records that match the criteria.
    """
    filtered_df = dataframe[dataframe[text_column].str.contains(fr'\b{word}\b', case=False, regex=True)]
    return filtered_df

"""### 4.2.1. Monopalabra general + negativo + neutro"""

plot_n_grams(
    dataframe=data,
    n_gram=1,
    title='Palabras m치s frecuentes en PRIMA (Global)',
    n_words=10
)

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Negativa'],
    n_gram=1,
    title='Palabras m치s frecuentes en PRIMA (NEGATIVOS)',
    n_words=10
)

"""- De forma general hay escontento por problemas con "devoluci칩n de dinero":
- Resaltan palabras como:
    - Devuelvan
    - Ladrones
    - Soles
"""

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Neutra'],
    n_gram=1,
    title='Palabras m치s frecuentes en PRIMA (NEUTROS)',
    n_words=10
)

"""- De forma general en los comentarios neutros resaltan palabras informativas como:
    - Saber
    - Quiero
    - Puedo
    - Favor (probablemente de "por favor")

### 4.2.1. Bi_gram negativo + neutro
"""

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Negativa'],
    n_gram=2,
    title='Palabras m치s frecuentes en PRIMA (NEGATIVOS)',
    n_words=10
)

"""Notas:
- Predominancia de conversaci칩n en torno a "devolver plata".
- De mediano impacto la menci칩n de los 17 a침os de Prima como AFP l칤der.
"""

# pd.set_option('display.max_colwidth', 150)

len(data.loc[data.valoracion=='Negativa'])

get_samples(
    dataframe=data.loc[data.valoracion=='Negativa'],
    text_column='mensaje',
    word='plazo fijo'
).mensaje.values
# ).shape
#.sample(2)

plot_n_grams(
    dataframe=data.loc[data.valoracion=='Neutra'],
    n_gram=2,
    title='Palabras m치s frecuentes en PRIMA (NEUTROS)',
    n_words=10
)

"""- En el caso del sentimiento neutro:
    - Predominancia de la conversaci칩n en torno a un contenido de seguridad relacionado a Stephen Jika y a la Sra Lovina.

### 4.2.2. N_gram + Campa침as principales
"""

filtered_data = data[data['categoria1'] != 'Sin_Categoria']

proporcion_categorias = filtered_data['categoria1'].value_counts(normalize=True)

cantidad_registros_por_categoria = filtered_data['categoria1'].value_counts()

print("Proporciones de categor칤as y cantidad de registros por categor칤a:\n")
print(proporcion_categorias)
print('\n','='*80)
print("\nCantidad de registros:\n")
print(cantidad_registros_por_categoria)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Campa침as') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(CAMPA칌AS + NEGATIVA)',
    n_words=10
)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Campa침as') & (data.valoracion=='Negativa')],
    n_gram=1,
    title='(CAMPA칌AS + NEGATIVA)',
    n_words=10
)

# get_samples(
#     dataframe=data.loc[(data.categoria1=='Campa침as') & (data.valoracion=='Negativa')],
#     text_column='mensaje',
#     word='17'
# )['mensaje'].values

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Aportes y fondos') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(APORTE Y FONDOS + NEGATIVA)',
    n_words=10
)

get_samples(
    dataframe=data.loc[(data.categoria1=='Aportes y fondos') & (data.valoracion=='Negativa')],
    text_column='mensaje',
    word='fondo viendo'
)['mensaje'].values

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Retiros coyuntura') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(RETIROS + NEGATIVA)',
    n_words=10
)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Retiros coyuntura') & (data.valoracion=='Negativa')],
    n_gram=1,
    title='(RETIROS + NEGATIVA)',
    n_words=10
)

# get_samples(
#     dataframe=data.loc[(data.categoria1=='Retiros coyuntura') & (data.valoracion=='Negativa')],
#     text_column='mensaje',
#     word='plata'
# )['mensaje'].values

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Marca') & (data.valoracion=='Negativa')],
    n_gram=2,
    title='(MARCA + NEGATIVA)',
    n_words=15
)

plot_n_grams(
    dataframe=data.loc[(data.categoria1=='Marca') & (data.valoracion=='Negativa')],
    n_gram=1,
    title='(MARCA + NEGATIVA)',
    n_words=15
)

"""- En marca se tiene el 56% de los comentarios totales categorizados, pero se resume en hate de m칰ltiples tipos no tan espec칤fico en el mensaje como otras categorias."""

# get_samples(
#     dataframe=data.loc[(data.categoria1=='Marca') & (data.valoracion=='Negativa')],
#     text_column='mensaje',
#     word='todas'
# )['mensaje'].values

"""# *5.Resumen del EDA*  
---  
- Para el **sentimiento negativo** hay una gran predominancia de palabras referentes a rentabilidad y a los 17 a침os de Prima liderando (ejemplo: "17 a침os robando mi plata... sinverg칲enzas" o "17 a침os enriqueciendo con la plata de los aportes"). Se sugiere tomar acci칩n para cambiar la percepci칩n de enriquecimiento a costa de los clientes que tiene la marca.
- Para el sentimiento neutro la combinaci칩n de palabras m치s frecuentes corresponde a solicitudes de informaci칩n. As칤 como buena recepci칩n de contenido de seguridad ("gracias") con la participaci칩n de Stephen Jika y Lovina Stephen.
- De las categorias tagueadas:
	- El 17% corresponde a **Campa침as** en su mayor칤a de traspasos, donde el sentimiento negativo se ve muy concentrado en torno a los 17 a침os de prima liderando y cuenta con alta presencia de comentarios de poop (游눨) adem치s de resumirse en "17 a침os robando".
	- El 11% corresponde a **Aportes y fondos**, donde el sentimiento negativo concentra combinaciones de palabras referentes a que "la rentabilidad en realidad es negativa".
	- El 7% corresponde a **Retiros**, donde la negatividad est치 centrada en la devoluci칩n de dinero y que "la afp no libera las 4 UIT's que tocan por ley".
-Entre las palabras frecuentes, se destac칩 la combinaci칩n **plazo fijo**, donde los usuarios mencionaron su preferencia por retirar dinero de la AFP y depositarlo en un plazo fijo. Esto sugiere la oportunidad de trabajar con productos similares o reforzar la confianza que genera un Fondo de Pensiones en comparaci칩n con un producto como el plazo fijo.

"""

!pip install nltk
import nltk
nltk.download('stopwords')

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from nltk.corpus import stopwords

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto' y 'valoracion'
data_negativa = data[data['valoracion'] == 'Negativa']

# Extraer los textos
textos_negativos = data_negativa['mensaje']

# Obtener las stopwords en espa침ol de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer trigramas
vectorizer = CountVectorizer(ngram_range=(3, 3), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_negativos)
trigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_trigrams = trigrams[top_indices]

# Crear una representaci칩n de texto de trigramas
trigrams_text = ' '.join(top_trigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(trigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

data_negativa = data[data['valoracion'] == 'Negativa']

# Extraer los textos
textos_negativos = data_negativa['mensaje']

# Obtener las stopwords en espa침ol de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer bigramas
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_negativos)
bigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_bigrams = bigrams[top_indices]

# Crear una representaci칩n de texto de trigramas
bigrams_text = ' '.join(top_bigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(bigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto', 'valoracion' y 'categoria1'
data_filtrada = data[(data['valoracion'] == 'Negativa') & (data['categoria1'] == 'Retiros coyuntura')]

# Extraer los textos
textos_filtrados = data_filtrada['mensaje']

# Obtener las stopwords en espa침ol de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer trigramas
vectorizer = CountVectorizer(ngram_range=(3, 3), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_filtrados)
trigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_trigrams = trigrams[top_indices]

# Crear una representaci칩n de texto de trigramas
trigrams_text = ' '.join(top_trigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(trigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto', 'valoracion' y 'categoria1'
data_filtrada = data[(data['valoracion'] == 'Negativa') & (data['categoria1'] == 'Retiros coyuntura')]

# Extraer los textos
textos_filtrados = data_filtrada['mensaje']

# Obtener las stopwords en espa침ol de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer trigramas
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_filtrados)
bigrams = vectorizer.get_feature_names_out()

# Sumar las frecuencias de cada trigrama y seleccionar los top 10
frequencies = X.toarray().sum(axis=0)
top_indices = np.argsort(frequencies)[::-1][:25]
top_bigrams = bigrams[top_indices]

# Crear una representaci칩n de texto de trigramas
bigrams_text = ' '.join(top_bigrams)

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                background_color ='white',
                stopwords = set(stopwords_es),
                min_font_size = 10).generate(bigrams_text)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from nltk.corpus import stopwords
import pandas as pd

# Suponiendo que 'data' es tu DataFrame y tiene una columna 'texto' y 'valoracion'
data_negativa = data[data['valoracion'] == 'Negativa']

# Extraer los textos
textos_negativos = data_negativa['mensaje']

# Obtener las stopwords en espa침ol de NLTK y convertirlas a una lista
stopwords_es = list(stopwords.words('spanish'))

# Utilizar CountVectorizer para extraer bigramas
vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords_es)
X = vectorizer.fit_transform(textos_negativos)
bigrams = vectorizer.get_feature_names_out()
frequencies = X.toarray().sum(axis=0)

# Crear un diccionario de bigrama-frecuencia
bigram_frequency = dict(zip(bigrams, frequencies))

# Definir una funci칩n para asignar colores
def color_func(word, *args, **kwargs):
    freq = bigram_frequency.get(word, 0)
    if freq > np.percentile(frequencies, 30):
        return 'red'  # Mayor frecuencia
    elif freq > np.percentile(frequencies, 20):
        return 'orange'  # Frecuencia media
    else:
        return 'yellow'  # Menor frecuencia

# Crear y mostrar la nube de palabras
wordcloud = WordCloud(width = 800, height = 400,
                      background_color ='white',
                      stopwords = set(stopwords_es),
                      min_font_size = 10,
                      color_func=color_func).generate_from_frequencies(bigram_frequency)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()